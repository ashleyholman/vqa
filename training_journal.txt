== 20th May, 11:50am ==
- Trained the model to 1 epoch and snapshotted.
- Resumed training for another 4 epochs.
- Noticed that the loss was increasing at each epoch:
epoch 1: 3.2247357749
epoch 2: 3.3537965479
epoch 3: 3.516193326
epoch 4: 3.6436337303
epoch 5: 3.7643052345

- The first epoch of training is definitely reducing loss, since the
  untrained model has ~0.02% accuracy on the validation set, but after 1 epochs
  training has 31% accuracy.

- Discussed with ChatGPT and we have two main ideas of what could be going wrong:
  1) Training from the 2nd epoch onwards was resumed from snapshot.  Possibly
  there could be a bug in restoring the optimizer state which is causing the
  loss to go up.
  2) The learning rate could be too high, causing training to overshoot.

- I am going to try the following:
 1) Train on 5 epochs without restoring from snapshot in the middle, to test the
    snapshot/restore bug hypothesis.
 2) If the above didn't fix the bug, try reducing learning rate from Adam's
    default (1e-3) to 1e-4.

Results:

1) Snapshot/restore functionality appears to work fine.  The same issue of
increasing loss occured when not using snapshotting.

Here's the results from the v1 model with 1e-3 learning rate:

epoch,training_loss,training_top_5_acc,training_accuracy,validation_f1_score_macro,validation_loss,validation_precision_macro,validation_top_5_accuracy,validation_recall_macro,validation_recall_micro,validation_accuracy,validation_precision_micro,validation_f1_score_micro,validation_top_5_acc
1,3.2315075886,68.928039445,31.3928569014,5.6813152488,3.0792971602,65.4738157559,73.724306521,6.445209268,35.1106114185,35.1106114185,35.1106114185,35.1106114185,
2,3.361339971,72.0446100005,32.8882699315,6.7326739821,3.4026299975,60.6931810296,73.1775474216,8.2992956059,33.0957201638,33.0957201638,33.0957201638,33.0957201638,
3,3.5159329014,73.0584531624,33.326347528,6.7345578681,3.7180197095,60.9107401213,73.6659917706,8.4431275852,33.6368810472,33.6368810472,33.6368810472,33.6368810472,
4,3.6597617971,73.6421059273,33.6738350043,7.5395065363,4.0672946342,57.4465725714,73.6212060423,8.6312476955,32.9497000289,32.9497000289,32.9497000289,32.9497000289,
5,3.7727548355,74.2246319495,34.0794624085,8.5137171176,4.08006612,52.5039431919,74.2570700803,9.9172454909,32.1006372636,32.1006372636,32.1006372636,32.1006372636,
6,3.8662004141,74.6751037167,34.2396852331,,4.2276296166,,,,,34.079606632,,,74.0816593112
7,3.9534126623,75.0180842218,34.4564705458,,4.2171596133,,,,,34.8535599989,,,75.0963359676
8,4.0262200343,75.3721068062,34.7911131543,,4.2361361302,,,,,36.1234220029,,,75.5106039542
9,4.0981813117,75.6558206406,35.0000112674,,4.7706815169,,,,,31.7400188473,,,72.5360851675
10,4.1495278687,75.9528300399,35.247443984,,4.7529160895,,,,,35.6335780998,,,76.0587626076

(missing metrics are because I added some new metrics after the code ran)

== Sun 21st May, 3pm ==
Reducing the learning rate saw the training and validation loss both steadily
trend down, so it looks like the 1e-3 learning rate was overshooting.

However, the new model (lr1e4) appears to be overfitting.  Results:

epoch,training_loss,training_f1_score_macro,training_precision_macro,training_top_5_accuracy,training_recall_macro,training_recall_micro,training_accuracy,training_precision_micro,training_f1_score_micro,validation_f1_score_macro,validation_loss,validation_precision_macro,validation_top_5_accuracy,validation_recall_macro,validation_recall_micro,validation_accuracy,validation_precision_micro,validation_f1_score_micro
1,2.5852090719,3.1206656457,68.8510755771,72.6861773448,2.7691301792,35.9672072779,35.9672072779,35.9672072779,35.9672072779,4.2924570786,2.1305200113,84.7486000743,78.8499398192,3.9149815897,40.9136288569,40.9136288569,40.9136288569,40.9136288569
2,2.073595188,8.4091373642,52.4334464158,79.9469078798,7.2375279044,40.8173842891,40.8173842891,40.8173842891,40.8173842891,9.2007364429,1.9639582348,72.5276034842,81.4405142894,8.0618229005,42.4512721946,42.4512721946,42.4512721946,42.4512721946
3,1.9220939211,13.60625916,49.7204741552,82.2749838312,11.3629908193,42.4394432088,42.4394432088,42.4394432088,42.4394432088,13.366798583,1.8953340608,56.506632693,82.3730837773,13.3805090139,43.0292880002,43.0292880002,43.0292880002,43.0292880002
4,1.8292348324,17.546353651,46.946821161,83.7974386883,14.5386174168,43.5772280775,43.5772280775,43.5772280775,43.5772280775,14.3463529594,1.8590599547,57.6302872128,82.967894231,13.4968189554,43.8447614693,43.8447614693,43.8447614693,43.8447614693
5,1.7616312745,21.3304292009,48.3829055154,84.8969143022,17.5368357974,44.4279188835,44.4279188835,44.4279188835,44.4279188835,13.8818796399,1.85231735,62.010898997,82.9524991369,12.219221176,44.3891879788,44.3891879788,44.3891879788,44.3891879788
6,1.7089118018,24.7722335595,49.0317632947,85.7753229808,20.2805201161,45.1224431389,45.1224431389,45.1224431389,45.1224431389,15.0596080459,1.8407398825,54.3733380235,83.2235460966,13.6280095327,44.3588643086,44.3588643086,44.3588643086,44.3588643086

Discussed with ChatGPT.  Since we have a lot of class imbalance, I want to try
adding weightings to the classes first.  We can also try regularization through
dropout, but I will try that later.  First I want to address the class imbalance
as it's quite severe:

[40728 40082 27889  6093  5745  4311  3073  2733  2512  2483  2346  1986
  1931  1927  1351  1117  1100   839   786   775   768   760   756   752
   691   684   641   637   615   532   526   476   466   462   454   451
 ...
     8     8     8     7     7     7     7     7     7     7     7     6
     6     6     6     6     6     5     5     4     4     4     4     3
     3     2     2     1]

I'm going to test changing my loss function to use class weights.

== Tue May 22 2023 ==
Added class weights.  The model's overall accuracy performance starts a lot
lower now compared to the previous model that quickly learnt to over-predict the
dominant classes.

Training accuracy after 6 epochs: 21% vs 45%
Validation accuracy after 6 epochs: 21% vs 44%

It's not clear that class weighting is actually something I should be using if I
just care about producing the best score possible on the test set.  Letting the
model learn the distribution of answers is resulting in much better performance.
However, I do want to try to improve my model's ability to actually learn the
VQA task in a general sense, so for now I will continue with weighted classes,
even if performance is worse, so that I can work on improving its actual ability
to learn.  Seeing the accuracy go up with weighted classes actually gives me
confidence that the model is truely learning how to answer the questions using
information from the image and text.

Later, when I want to try to get as high a score as possbile, I might reduce the
weighting, either remove it entirely or scale it back so they are partially
weighted.

== Tue 23rd May 2023 ==
Took a detour to implement fully automated resuming of training, and validating
of unvalidated snapshots, so that we are resilient to spot instance
interruption. This will accelerate the pace of my experimentation as I no longer
need to closely monitor training runs.

This involved adding DDB records for snapshot metadata to make them queryable,
so we can easily find new snapshots pending validation, as well as determine the latest
epoch we've trained up to, for resuming training.

After that, implemented dropout to try to address overfitting.  It hasn't seemed
to make much different when I compare the before and after.

== Wed 24th May 2023 ==

Added batch normalization.  Didn't make any noticeable difference.

Then, I had a thought.  Since I'm using answer classes, and each answer is only
represented by a different index in my final layer, the model has no knowledge
of the actual text corresponding to the answer.  As a human, I'm imagining how
hard it would be to do a multiple choice test where the answer text was hidden,
and you just had to figure out for yourself what each box might represent.  I
figured the model would benefit from actually knowing what the text was for each
answer.

So, upon discussion with ChatGPT, we decided to use BERT to generate a static
embedding for each answer class from its answer text.  Then, in our model,
instead just passing the question and image embeddings through a final linear
layer to produce logits, we instead produce our own embedding (of length 768;
same size as BERT) to be compared with the answer embeddings.  Then, the final
prediction layer computes a cosine similarity score from our model's embedding
to the 1000 answer embeddings.

The idea is to provide the model with semantically meaningful representations of
the answer classes, to help it not only understand that two classes are
different, but *how* they are different on a semantic level.

Now I trained this, but the accuracy is staying very low but moving around in an
unstable way.  Perhaps the learning rate needs to change now.

However, I'm going to take a break from training to implement an optimization
idea.  Rather than computing the BERT and ViT embeddings in the forward pass of
our model, since these weights are frozen I should be able to pre-compute them
once and store to disk.  Then keep them all in memory during training and just
feed the embeddings directly into my model.  Should hopefully be a huge speedup
for training.

== Thu 25th May 2023 ==
Spent most of the day implementing the speedup idea from yesterday.  Now the
question text embeddings and image embeddings are pre-computed and stored in the
data directory in "<dataset>_embeddings.pt".  These are loaded by the dataset
when it is initialised (or generated and stored if the file isn't yet present).
The embeddings files will also be bootstrapped from S3 on node startup to avoid
having to generate them again.

Now, the image and question embeddings are fed directly into the model.  The
model no longer has BERT and VIT in its forward pass.  As a result, training has
gone down from ~36 mins per epoch to ~4 seconds.  Validation has gone from ~15
mins per epoch to ~2 seconds.  Also, snapshot sizes are down to ~34MB now as
they no longer contain ViT/BERT weights.

At first, the optimization didn't have as much impact as I was hoping, but then
I kept increasing batch size more and more and it kept getting quicker.  Ended
up with batch size 5000.  Looks like I can fit huge batches on the GPU now given
that I have far less parameters in the model.

In some ways it's embarassing that such a huge and in-hindsight obvious
optimization was available, yet I had already spent days of GPU time training
very slowly.  However it only took a few days to realize, so it's not that bad :)

At some point I might want to explore finetuning the ViT/Bert weights, which
will require going back to slow training, but I'll save that for later once I've
found the best architecture for my layers on top.

This increase in training speed has really amazed me because now my
possibilities for training models has gone way up.  My previous models where I
waited overnight to get 10 epochs, could now be trained for 5000+ epochs in the
same time period, so I'll be able to go back and explore some of those other
architectures to see where they top out in terms of their performance metrics.

In terms of actual model architecture, I did get some results from training the
new "answer embeddings" architecture, and the performance is shockingly bad, and
the model just doesn't seem to be able to learn anything.  Accuracy starts at
0.2% and barely can make it over 0.5% with further training.  I experimented
with larger learning rates but still couldn't get it to improve much. I also
tried adding an additional hidden layer in case the model didn't have enough
capacity, but the results only increased slightly.  I could experiment with more
layers or using a transformer.  However, given how bad the accuracy is, and how
slow the learning is occuring, it might be a better option to go back to using a
simple linear classifier, as that showed much better results.  Possibly the
problem of producing a BERT embedding that represents the answer, is just a way
harder problem than predicting an output class, but I would need to explore that
more to understand exactly why it's bad.

I'm not ready to give up entirely on answer embeddings yet, but I think I'll go
back to the simple classifier first to see what results I can get with my hugely
increased ability to train for large numbers of epochs in a short time.

== Fri May 26th 2023 ==
I had a breakthrough on getting answer embeddings to work.  The answer
embeddings model seems to be roughly as capable as the simple classifier,
although takes longer to train.

The breakthrough happened when I started wondering if perhaps the answer
embeddings might all be clustered around a small area of the BERT embeddings
space, since they're mostly single words and many of them nouns etc, in which
case the "cosine similarity" scoring that we output, would not have the ability
to strongly predict one class over the other.

I discussed with ChatGPT, and it said that it's a common practice to normalise
embeddings before doing cosine similarity, however there is some risk that we
lose information since BERT embeddings are quite rich with semantic meaning.

I also noticed that the code I was using was already doing F.normalize() on the
answer embeddings, but GPT said this was just a vector normalization, which
changes all of their length to 10, but does not change the angles between
vectors, which is what cosine similarity looks at.  It recommended I look at
z-normalization.  It also suggested I could test my hypothesis of embeddings all
being close to one another by computing the pairwise cosine similarity between
all the answer classes.

I ended up going strait to implementing z-normalisation without doing the cosine
similarity analysis.  The results were immediately obvious that it fixed my
model.  I would like to return to the suggestion of checking the similarity
scores on the raw answer embeddings, but I'll put that on my TODO backlog.

The rest of my day was mostly focused on refactoring any model or
training-related configuration/hyper-parameters into a new class,
ModelConfiguration, that loads all settings from a YAML file.  So now to tweak
my model architecture or training settings, I can just modify that file.  Should
allow for more rapid experimentation on different models and hyper-params.

== Sat 27th May 2023 ==
Didn't do much today, except I have an idea in my head that I want to do a
pretty big overhaul of how training/validation/snapshots etc work.  I want to
introduce the concept of a "run" which will be identified by a UUID (referred to
as "run ID") and has the following attributes:
- A configuration set (key/value pairs that determine the model architecture and
  hyperparameters, including the max number of epochs that the run should train
  for).
- Git commit hash from the code that the run will execute on
- Timestamps of when the run started and ended.

The DDB records for snapshots and performance statistics will then be associated
with a Run ID instead of a model name.

I'll make a tool to generate an index page for my graphs so that I can see all
the different runs with a summary of key metrics, and click on them to view the
detailed charts.  I would also like to add an option to compare two runs
side-by-side on the same charts to make it easier to see the impact of a change.

Eventually, generation of these graphs and indexes in real time can be automated
via lambda and stored in a public S3 bucket for easy online viewing.

The new approach will deprecate the train_model.py and test_model.py scripts.
Previously I was manually invoking those to orchestrate my own runs, and
recently add the do_work.py script that runs the two of them in an infinite
loop.  That approach was cumbersome because the current model would train
infinitely until I killed the script, which resulted in models trained way too
far into overfitting, and different models having different numbers of epochs
depending on when I killed the script, which made it hard to compare the charts
side-by-side, and didn't offer any possibility of queueing up multiple runs.

Now I will make a new script, run.py, and add a max_epochs setting in the config
settings.  run.py will do all of the training and validation up until the max
epochs.  Eventually, I could make a system where I push a new model
configuration set into an SQS queue, then my EC2 autoscaling group can scale
based on the presence of work in queue (eg. only launch an ec2 instance when
there's new run(s) that need doing, and then shut it down after the run(s)
finish).  run.py will also need the ability to resume a partially complete run
from the latest snapshot, which will be necessary in cases where we suffered a
spot interruption.  This architecture might even enable some fancier use cases
like a distributed grid search for the optimal model
architecture/hyperparameters.

For today, I just refactor train_model.py into a new ModelTrainer class so that
it can later be used by run.py.

== Sat 3rd Jun ==
Got my run.py system going.  Now can more rapidly experiment.

Still need to make graph_performance.py produce an index page showing recent
runs ordered by time, with their model configuration attributes.

Also would be helpful to have a mechanism where I can submit a batch of runs I'd
like to execute, and have the instance automatically execute them in sequence.
That way I could queue up a bunch of different experiments and let them go
overnight.

For now, have done a few runs on older model architectures up to 50 epoch and
found they didn't actually top out.  They keep learning and improving on the
validation set.  I'll have to run some longer runs up to 200 epochs next.
Interested to see whether the answer embeddings approach can eventually reach
and exceed the performance of the simple classifier.
