- data analysis tooling
- explore automated stopping
- explore dynamic learning rate
- explore how to find optimal learning rate
- Implement queue for runs.  Submit config file to queue.  Implement autoscaling
  based on queue to spin up and then shut down instances based on pending runs.
- Implement automated lambda-based report generation and uploading to S3?  So I
  can just refresh my web browser to see the results.
- Check cosine similarity between raw answer embeddings to confirm clustering
  hypothesis
- React app:
  * Sample questions page:
    * show totals for true positive, false positive etc
  * Add confusion matrix
  * When mousing over tables, highlight the row
- Model:
  * More substitutions: eg. "playing game" -> "playing", "ones on right", "one on right" -> "on right"?
  * Try different embeddings: bert-huge and llama
  * try https://huggingface.co/google/vit-huge-patch14-224-in21k (2.53GB)
  * try https://huggingface.co/openai/clip-vit-large-patch14 (1.71GB)
  * try https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K/ (5.47GB)
  * note: CLIP can be used for *both* image and text embeddings, and they are in
    a shared space.
- Implement S3 store/retrieval directly into EmbeddingsManager
- Explore making fetch_datasets.py parallell.  One process per download.  So
  that unzipping can use multiple cores at once.
- Speed up generate_web_json.py.  Avoid fetching metrics for runs that we don't
  need to update.  Also add explicit --run-id option for single run update only.
- Add pending runs and ability to auto-scaledown ASG when no pending runs remaining
- Rethink the whole json generation -> local react -> web deploy system
  * generation taking too long, rewriting too much, fetching ddb too much
  * upload to S3 too slow and syncing rewrites too many existing files
- Make ops tool for changing run record attributes (eg. change run_status)
