###### model name ######
model_name: 'vitbase_1e5_unweighted_1hidden_hs1000_dropout_batchnorm_notransform'

###### model architecture-related settings ######
num_hidden_layers: 1
hidden_size: 1000
use_answer_embeddings: false
use_answer_embedding_z_normalization: true
use_dropout: true
dropout_input_probability: 0.2
dropout_hidden_probability: 0.5
use_batch_normalization: true
use_gating: false
# whether to include a linear transform layer for each input embedding before combining them.
transform_input_embeddings: false

###### models to use for input embeddings ######
input_embedding_model_names:
  vision: 'google/vit-base-patch16-224-in21k'
  text: 'bert-base-uncased'

###### training-related hyperparameters ######
batch_size: 5000
learning_rate: 1.e-5
shuffle: true

###### loss-function parameters ######
use_class_weights: false

###### dataset options ######
merge_singular_plural_answer_classes: true

###### epoch-related settings ######

# max_epochs: Training will cut off here if it hasn't already cut-off from another condition.
max_epochs: 800

# snapshot_every_epochs: How frequently to take a snapshot.  This will be the
# restore point if our run is interrupted, for example by spot instance eviction.
snapshot_every_epochs: 100

# metrics_every_epochs: How frequently to calculate performance metrics for both
# the training and validation set.  This will determine the resolution of
# graphs.  Tracking performance adds an overhead on training and requires doing
# an inference run over the validation set.
metrics_every_epochs: 5
